import os
import pandas as pd
import streamlit as st
import json
import io
from model_llm import ChatModelLLM
from model_gemini import ChatModelGemini
from context import DEFAULT_CONTEXT
import rag_llm

FILES_DIR = os.path.normpath(
    os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "files")
)

@st.cache_resource
def load_model(answer_model_option):
    model = None
    if answer_model_option == "gemma-2b-it":
        model = ChatModelLLM(model_id="google/gemma-2b-it", device="cpu")
    else:
        model = ChatModelGemini(model_id="gemini-1.5-pro", device="cpu")
    return model

@st.cache_resource
def load_encoder(embedding_model_option):
    encoder = None
    if embedding_model_option == "all-MiniLM-L12-v2":
        encoder = rag_llm.Encoder(
            model_name="sentence-transformers/all-MiniLM-L12-v2", device="cpu"
        )
    else:    
        encoder = None
    return encoder

def process_files(uploaded_files, encoder):
    file_paths = []
    DB = None
    if uploaded_files:
        with st.spinner("Processing the files..."):
            for uploaded_file in uploaded_files:
                file_paths.append(save_file(uploaded_file))
            if uploaded_files != []:
                docs = rag_llm.load_and_split_pdfs(file_paths)
                print(docs)
                DB = rag_llm.FaissDb(docs=docs, embedding_function=encoder.embedding_function)
        st.success("Files processed successfully!")
    return DB    

def save_file(uploaded_file):
    file_path = os.path.join(FILES_DIR, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def handle_user_input(prompt, model, DB, k, max_new_tokens):
    if not prompt.strip(): 
        combined_prompt = DEFAULT_CONTEXT
    else:  
        combined_prompt = f"{prompt}\n\n{DEFAULT_CONTEXT}"
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt if prompt.strip() else "Using default context.")

    df = None
    with st.chat_message("assistant"):
        context = None if not DB else DB.similarity_search(combined_prompt, k=k)
        answer = model.generate(combined_prompt, context=context, max_new_tokens=max_new_tokens)
        print(answer)
        try:
            cleaned_json = answer.strip().replace("```json", "").replace("```", "").strip()
            data = json.loads(cleaned_json)
            df = pd.DataFrame(data)
            st.dataframe(df)
        except Exception as e:
            st.error(f"Error processing JSON data: {e}")
        # st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": df})

def setup_page():
    st.set_page_config( 
        page_title="AI Auto Test",  
        page_icon="ðŸ’¬",  
        layout="centered"
    )

def display_title():
    st.title("ðŸ’¬ AI Auto Test")
    st.caption("ðŸš€ AI Automatic Test Case Generation")

def display_sidebar():
    with st.sidebar:
        st.header("Model Configuration")

        # Radio button to choose embedding model
        embedding_model_option = st.radio(
            "ðŸ”¤ Choose an Embedding Model",
            options=["all-MiniLM-L12-v2", "gemini-1.5-pro"],
            index=0, # Default to first model
        )

        encoder = load_encoder(embedding_model_option)

        # Radio button to choose answer generation model
        answer_model_option = st.radio(
            "ðŸ¤– Choose an Answer Generation Model",
            options=["gemma-2b-it", "gemini-1.5-pro"],
            index=1, # Default to first model
        )

        model = load_model(answer_model_option)  

        max_new_tokens = st.number_input("max_new_tokens", 128, 4096, 512)
        k = st.number_input("k", 1, 10, 3)
        uploaded_files = st.file_uploader(
            "Upload PDFs or Word documents for context", type=["pdf", "doc", "docx"], accept_multiple_files=True
        )

        DB = process_files(uploaded_files, encoder)


        return model, max_new_tokens, k, DB     

def initialize_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = []

def display_chat_messages():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                st.dataframe(message["content"])
            else:
                st.markdown(message["content"])

def main():
    setup_page()
    display_title()

    model, max_new_tokens, k, DB = display_sidebar()

    initialize_chat_history()

    display_chat_messages()

    if prompt := st.chat_input("Ask me anything!"):
        handle_user_input(prompt, model, DB, k, max_new_tokens)

if __name__ == "__main__":
    main()